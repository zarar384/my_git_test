
1) Monolith плохо масштабируется по командам и по скорости изменений.
Microservices увеличивают гибкость, но сильно усложняют наблюдаемость.

Связь microservices ↔ CI/CD ↔ Observability(diagnosis)
Monolith ≈ Waterfall: Долгий цикл: requirements => design => code => test => deploy
Microservices ≈ CI/CD: Короткий цикл: plan => code => build => test => deploy => measure => repeat
CI/CD без observability - опасно.
Чем чаще деплой - тем больше нужен мониторинг и трассировка.

Почему Observability ОБЯЗАТЕЛЬНА в microservices? В microservices нельзя понять, что происходит, без observability.

Вывод: Observability is essential for microservices because failures are inevitable, deployments are frequent, and system behavior cannot be understood without metrics, logs, and traces.
или же: The more microservices you have, the more observability you need.

Вопросы:
1. Почему microservices требуют observability? много сервисов; сложные зависимости; distributed failures; невозможно дебажить без метрик, логов и трейсов.
2. Почему CI/CD увеличивает требования к мониторингу? частые деплои; каждый деплой несёт риск; нужно быстро обнаруживать деградации и откатываться.
3. Почему monolith проще мониторить? один процесс; одна база; один лог; один entry point.
4. Что происходит без observability? blind deployments; долгое MTTR; сложно найти root cause; нестабильная система.

2) Monitoring - это: регулярный сбор и визуализация runtime-данных системы, чтобы отслеживать её здоровье.
!regularly!

Три ГЛАВНЫХ вопроса мониторинга:
* Is the service ON? Обычно проверяется через: health checks; ping / HTTP HEAD / GET.
* Is the service FUNCTIONING as expected? thresholds или же 'Если ошибок < 5 в минуту => считаем нормой'.
* Is the service PERFORMING well?  'HTTP response time ≤ 20 ms => OK'

Три ГЛАВНЫХ вопроса по аналогии с машиной:
* ON => двигатель крутится (RPM > 0)
* FUNCTIONING => машина едет
* PERFORMANCE => нет warning lights, нормальные показатели

Telemetry data - это данные, которые мы собираем для мониторинга: метрики; логи; события; трейсы.
!Telemetry data показывает ГДЕ проблема, но не всегда ПОЧЕМУ!

Monitoring ≠ Debugging
Monitoring(detection): указывает на аномалию; показывает симптомы.
Debugging: ищет root cause; требует логов и трейсов.

Без мониторинга microservices превращаются в black box.

DevOps метрики:
* MTD - Mean Time to Detection: Среднее время от начала инцидента до его обнаружения.
показывает: качество мониторинга; качество алертов.
* MTR / MTTR - Mean Time to Resolve: Среднее время от обнаружения проблемы до её полного устранения.
Что показывает: качество observability; качество процессов; зрелость команды.

Вопросы:
1. Какие вопросы должен отвечать мониторинг? Is the service up? Is it working correctly? Is it performing well?
2. В чём разница между monitoring и observability? Monitoring tells you that something is wrong, observability helps you understand why.
3. Что такое telemetry data? Runtime data (metrics, logs, traces) used to monitor system health and behavior.
4. Что такое MTD и MTTR? MTD => time to detect a problem; MTTR => time to fix it.

3) Какие ключевые метрики собираются для Monitoring? Собирать нужно не 'всё подряд', а только meaningful metrics.
Система обычно состоит из 3 слоёв:
* UI layer (Web / Mobile)
* Service layer (microservices)
* Infrastructure layer (CPU, memory, disk, network)
Под каждый слой - свои метрики.

* RED Method (Service + частично UI) - Request-driven metrics, где RED = Rate, Errors, Duration - cамый популярный метод для microservices.
RED method focuses on how a service behaves from the user’s perspective.
R - Rate (Throughput): Requests per second; Traffic volume. - 2M requests/sec
E - Errors: HTTP 5xx; business errors; fatal exceptions. - Смотрим error rate, а не просто количество.
D - Duration (Latency): Response time; P95 / P99 latency. - Важнее перцентили, а не average.

* USE Method (Infrastructure layer) - Resource-driven metrics, где USE = Utilization, Saturation, Errors- используется для хостов, контейнеров, VM.
USE method helps identify infrastructure bottlenecks.
U - Utilization: CPU %; Memory %; Disk usage %.
S - Saturation: Очереди; Backpressure; Resources at 100% - Network queue length; Thread pool queue.
E - Errors: Disk write errors; Network errors.

* Four Golden Signals (Google SRE) - If you can measure only four metrics for a user-facing system, measure these.
Golden Signals(RED + Saturation):
- Latency
- Traffic (Throughput)
- Errors
- Saturation

* Core Web Vitals (UI / Web ONLY) - важно для frontend и SEO - UX метрики
Core Web Vitals напрямую влияют на SEO - Google ранжирует сайты с учётом этих метрик.
- LCP - Largest Contentful Paint - Время, когда пользователь чувствует, что страница загрузилась.
- FID - First Input Delay - Когда пользователь может впервые взаимодействовать; perceived responsiveness.
- CLS - Cumulative Layout Shift - Насколько страница 'прыгает'; perceived stability.

Вопросы:
1. Какие методы мониторинга ты знаешь? RED method; USE method; Golden Signals; Core Web Vitals.
2. Чем RED отличается от USE? RED => service-level, user-facing; USE => infrastructure-level.
3. Что такое Golden Signals? Latency, traffic, errors, saturation - ключевые метрики Google SRE.
4. Почему average latency - плохая метрика? Потому что она скрывает хвосты, важнее P95/P99.
5. Зачем нужны Core Web Vitals?  UX; SEO ranking; perceived performance.

4) Observability. 
Monitoring - часть observability, но не наоборот. Они не конкурируют, а дополняют друг друга.

Monitoring vs Observability - отличие:
* Monitoring - Симптомы - Нужно заранее знать, ЧТО мониторить
Работает по принципу - Реактивный подход: 'Следим за CPU'; 'Следим за error rate'.
Хорошо отвечает на вопросы: WHEN (когда сломалось); WHERE (где сломалось).
Подходит для: monolith; систем с предсказуемыми точками отказа.
* Observability - Причины - Позволяет находить проблемы, о которых мы не подумали заранее и собирает actionable data со всей системы.
Даёт holistic view. 
Работает по принципу - Проактивный подход.
Хорошо отвечает на вопросы: WHY (почему это произошло).
Подходит для: microservices; distributed systems; сложных CI/CD систем.

Monitoring vs Observability по аналогии с машиной:
Monitoring - приборная панель - Видим, что что-то не так, но не знаем почему: RPM; скорость; warning lights.
Observability - диагностика у механика - Понимаем root cause, а не симптом: подключение к OBD; получение детальных данных; точное понимание причины.

Термины:
Tool sprawl - ситуация, когда: используется много разрозненных monitoring tools; нет единой картины системы; данные не коррелируют друг с другом.
Observability решает проблему Tool sprawl, объединяя: metrics; logs; traces.

Ключевая концепция: Unknown Unknowns - Observability помогает обнаружить unknown unknowns -
проблемы, о которых мы даже не знали, что они возможны.

Вопросы:
1. В чём разница между monitoring и observability? Monitoring tells us when and where something is wrong, observability helps us understand why it is wrong.
2. Почему monitoring недостаточно для microservices?  слишком много сервисов; сложные зависимости; непредсказуемые точки отказа; нужны коррелированные данные.
3. Что такое 'unknown unknowns'? Issues that we didn’t anticipate or define monitoring for in advance.
4. Что такое tool sprawl? Using many disconnected monitoring tools without a unified view of the system.

5) Методы сбора метрик (Push vs Scrape)
Существует 2 основных метода сбора метрик и сохранения их в time-series database: Push; Scrape.

* Push method - Метрики отправляются приложением в time-series DB или в агент.
- Приложение / микросервис сам отправляет метрики
- Используются протоколы: TCP / UDP / HTTP
- Часто используется agent / daemon для агрегации
Напр. Application => StatsD (agent) => Graphite (TSDB)

Зачем нужен агент (StatsD): 
- TSDB потребляет CPU и storage
- хранить каждую метрику дорого
- агент: принимает метрики; агрегирует (avg, sum, count); отправляет уже агрегированные данные.
!Push = application-driven metrics!

Плюсы:
+ удобно для: ephemeral сервисов; IoT; environments без inbound доступа.
+ приложение контролирует отправку

Минусы:
- сложнее контролировать нагрузку
- риск потерять метрики
- сложнее стандартизировать

* Scrape method - Метрики читаются извне системой мониторинга:  
- Приложение экспортирует метрики через endpoint; 
- Time-series DB сама приходит и читает метрики.
Напр. Application (/metrics) ← Prometheus
Принцип работы: приложение предоставляет HTTP endpoint => Prometheus знает адрес сервиса => Prometheus периодически scrape-ит метрики.
!Scrape = monitoring-driven metrics!

Плюсы:
+ централизованный контроль
+ легко масштабировать
+ стандарт для Kubernetes / cloud-native
+ Prometheus ecosystem

Минусы:
- нужен network access
- сложнее для: IoT; edge; NAT / firewalls.

!Push vs Scrape - это выбор, а не религия!
!Не надо слепо выбирать Prometheus только потому, что 'так делают все'!

Перед выбором нужно подумать:
- Тип систем: custom applications? OS / DB / infra?
- Scalability: новые сервисы? новые регионы? multi-geo?
- Complexity: IoT? edge devices? distributed world-wide?
- Network ограничения:inbound доступ есть? firewall / NAT?

Выбор зависит от: архитектуры; масштабов; окружения.

Вопросы:
1. Какие методы сбора метрик ты знаешь? Push и Scrape.
2. В чём разница между Push и Scrape? Push - приложение отправляет метрики. Scrape - система мониторинга читает метрики.
3. Пример Push-модели? Application => StatsD => Graphite.
4. Пример Scrape-модели? Prometheus => /metrics endpoint приложения.
5. Почему используют агрегацию в Push? Чтобы снизить нагрузку на TSDB и стоимость хранения.
6. Почему Prometheus популярен? Scrape model, cloud-native, Kubernetes-friendly.
7. Когда Push лучше Scrape? IoT; edge devices; нет inbound network доступа.
8. Главная ошибка при выборе Prometheus? Использовать его 'потому что модно', а не потому что подходит архитектуре.

6) Telemetry data (MELT), где M - Metrics; E - Events; L - Logs; T - Traces. 
В observability - 4 типа telemetry data.
* Events - это действие, которое произошло в конкретный момент времени.
- есть action
- есть timestamp
- показывает факт того, что что-то произошло
Напр. vending machine: покупка пачки чипсов в 3:20 PM
Где встречаются: event streaming platforms (Kafka); event-driven microservices.
!event подтверждает, что ожидаемое действие произошло!
!но не показывает частоту!
!Event ≠ metric!

* Metrics - это агрегированное значение событий за период времени.
- считает события
- агрегирует (count, rate, avg)
- всегда привязана ко времени
Напр. vending machine: 100 пачек чипсов в минуту
Зачем нужны: сравнение во времени; выявление трендов; деградация / рост.
Напр: сегодня: 100/мин => неделю назад: 200/мин => видим проблему, но не причину

* Logs - это детализированное описание события.
- содержит контекст
- содержит metadata
- гораздо подробнее, чем event
Напр. vending machine: продукт: chips; цена: $2; время: 3:20 PM; vending machine ID; локация: Sydney; payment method: Mastercard
!Logs = максимум деталей!

* Traces - это путь запроса через несколько сервисов.
- используется в microservices
- показывает взаимодействие сервисов
- помогает найти точку сбоя
Напр. vending machine => bank => Mastercard => bank => vending machine
!любой шаг может упасть - trace показывает где именно!
!Traces отвечают на вопрос: где запрос сломался!

* Связь между MELT
- Event - факт действия
- Metric - агрегированная статистика событий
- Log - подробности одного события
- Trace - путь запроса через систему

Вопросы:
1. Какие типы telemetry data ты знаешь? Metrics; Events; Logs; Traces (MELT).
2. Чем event отличается от metric? Event - одно действие в момент времени. Metric - агрегированное значение событий за период.
3. Зачем нужны metrics, если есть events? Metrics позволяют видеть тренды и сравнивать во времени.
4. Чем log отличается от event? Log содержит гораздо больше контекста и деталей.
5. Для чего нужны traces? Чтобы увидеть путь запроса через микросервисы и найти точку отказа.
6. Какие данные помогают найти root cause? Logs и traces (не metrics).

7) Prometheus: как собирать метрики (Exporters, Scrape, Push Gateway):
Prometheus (хранит и обрабатывает метрики) -- pull --> scrape (читает /metrics endpoint(HTTP GET)) --> Exporter (собирает и преобразует метрики) -- reads --> System (источник данных)

Базовая идея: Prometheus - это pull-based time series database.
!В Prometheus НЕЛЬЗЯ напрямую пушить метрики!

Проблема: не всегда есть доступ к source code системы, с которой нужно собрать метрики.
Примеры: базы данных (MySQL, PostgreSQL), OS (Linux, Windows), CloudWatch (AWS), HAProxy, IoT (датчики, сенсоры).
!Мы не можем: менять код MySQL, обновлять код миллионов IoT, позволить миллионам устройств пушить метрики в Prometheus (он задохнётся).

Антипаттерн: bash / PowerShell скрипты, cron jobs / scheduled tasks, ручной сбор и отправка метрик.
Минусы: не масштабируется, сложно поддерживать, нестабильно.

* Exporters - верное решение - это компонент, который: собирает метрики из системы и отдаёт их в формате, понятном Prometheus.
!Exporter = адаптер между системой и Prometheus!
Он может быть как установлен на системе (Linux exporter), так и установлен рядом (CloudWatch, HAProxy, IoT).
Примеры: Node Exporter (Linux), Windows Exporter, MySQL Exporter, CloudWatch Exporter, HAProxy Exporter.


* Scraping - это процесс, когда Prometheus подключается к exporter, читает метрики, сохраняет их у себя.
!Scrape = Prometheus pulls data!
- настраивается в prometheus.yml
- default scrape interval = 15 секунд
- Prometheus сам контролирует частоту и нагрузку

Scraping идеален, когда: 
- heterogeneous infrastructure
- много разных компонентов
- OS + DB + proxies + cloud services
- централизованный контроль 

* Push Gateway => часть Prometheus ecosystem, который содержит встроенный exporter - иногда приложение хочет отправить метрики само.
!Push Gateway НЕ делает Prometheus push-based!
Принцип работы:
- application => Push Gateway
- Push Gateway хранит метрики временно
- Prometheus scrapes Push Gateway
Используется для: batch jobs, short-lived jobs, cron jobs.
!Prometheus ВСЕГДА pull!

Вопросы:
1. Как Prometheus собирает метрики? Pull model (scraping).
2. Что такое exporter? Компонент, который собирает метрики из системы и отдаёт их Prometheus.
3. Зачем нужны exporters? Когда нет доступа к source code системы.
4. Что такое scraping? Процесс, когда Prometheus периодически читает метрики у exporter.
5. Почему нельзя напрямую пушить метрики в Prometheus? Prometheus архитектурно pull-based.
6. Зачем нужен Push Gateway? Для batch / short-lived jobs.
7. Делает ли Push Gateway Prometheus push-based? Нет. Prometheus всё равно scrapes.
8. Почему Push Gateway не рекомендуют для long-running services? Потеря семантики, stale metrics, проблемы с масштабированием.

8) Node Exporter - это официальный Prometheus exporter для сбора метрик Unix-based систем - exporter для Unix-based операционных систем.
!Node Exporter НЕ имеет отношения к Node.js!
Node = любая машина с Unix-based kernel: Linux, ubuntu, other Unix-like OS.
Является частью Prometheus project и поддерживается комьюнити, считается стандартом для инфраструктурных метрик
!есть official exporters, а есть third-party exporters, написанные другими компаниями и разработчиками.

Метрики(инфраструктурные метрики (USE method):
- CPU usage
- Memory usage
- Disk usage
- Network I/O
- Filesystem
- Load average
- Process / kernel metrics
!Это метрики уровня ОС, а не приложения!
!Node Exporter - foundation для infra monitoring!

!!! ДЛЯ .NET WINDOWS SERVER НУЖЕН Windows Exporter!
ПРЕДСТАВИМ, ЧТО РАБОТАЕТ НА ЛИНУКСЕ БУЭЭЭ СЕРВЕРЕ, ТОГДА:
[ .NET Application ]
  -- OpenTelemetry => Prometheus / Grafana
  |     (requests, errors, latency)
  |
[ Linux Server ]
  -- Node Exporter => Prometheus / Grafana
        (CPU, RAM, Disk, Network)
НО ЭТО ТАК СЛОЖИЛОСЬ ИСТОРИЧЕСКИ... СЕЙЧАС  ВМЕСТО ШИНДОУСОВ ВСЕ ЧАЩЕ ИЗ-ЗА КОРА ПРИЛОЖЕНИЯ РАЗВОРАЧИВАЮТ НА ЛИНУКС СЕРВАКАХ... ИЗ-ЗА КОНТЕЙНЕРОВ, КУБЕРНЕТИСОВ ВСЯКИХ. КЛАУДЫ ДЕШЕВЛЕ И ПРОЧАЯ ДЕВОПС МИШУРА, КОТОРАЯ РЯДОВОМУ ПРОГРАММИСТУ ДО ЛАМПОЧКИ. Т.Е ЕСЛИ У ТЕБЯ ЛЕГАСИ ГОВ... КОД... ТО ТАМ СКОРЕЙ ВСЕГО ВИНДА НА СЕРВАКЕ. 

.NET раньше => Windows Server + IIS
.NET сейчас => Linux + Docker + Kubernetes

Вопросы:
1. Что такое Node Exporter? Официальный Prometheus exporter для Unix-based систем.
2. Имеет ли Node Exporter отношение к Node.js? Нет.
3. Какие метрики собирает Node Exporter? CPU, memory, disk, network, filesystem, kernel metrics.
4. На каком уровне работает Node Exporter? Infrastructure / OS level.
5. Что значит 'official Prometheus exporter'? Часть Prometheus project и поддерживается его сообществом.
6. Можно ли расширять Node Exporter? Да, через pluggable collectors.

9) Prometheus data model (metrics, labels, time series) - в Prometheus все данные хранятся как time series -
значение метрики (timestamp (Unix timestamp))
!Prometheus = time series database!

Каждая time series в Prometheus уникально идентифицируется:
- metric name
- labels (key=value pairs) - способ разделять одну метрику на разные time series
	metric_name{key="value", key="value"}, где
	- labels в { } и могут быть 0, 1, n.
	Пример: для метрика auth_api_hit с labels: auth_api_hit{count="1", time_taken="800"}, где
	- auth_api_hit - metric name, count="1" - label, time_taken="800" - label
	!Одна метрика => много time series через labels!
Labels используются для: фильтрации, агрегации, группировки.
!Metric name + labels = unique time series!

!Prometheus не хранит 'объекты', он хранит числовые значения во времени.

Вопросы:
1. Как Prometheus хранит данные? Как time series (value + timestamp).
2. Что делает time series уникальной? Metric name + set of labels.
3. Что такое label в Prometheus? Key-value пара, описывающая измерение метрики.
4. Можно ли иметь метрику без labels? Да.
5. Зачем нужны labels? Для фильтрации, агрегации и разделения метрик.
6. Что важнее: metric name или labels? Оба: metric name определяет метрику, labels - её измерения.

10) Prometheus data types (Scalar, Instant Vector, Range Vector)
Prometheus имеет собственный query language - PromQL.

* Scalar - одно значение (float, string).
Пример метрики:
prometheus_http_requests_total{code="200", job="prometheus"}

prometheus_http_requests_total{job="prometheus", code=~"2.*"}
code=~"2.*" - код начинается с 2, например 200,201,204,205.
!Такую гибкость float не даёт!
! если написать code=200 - ничего не вернет - Label values в Prometheus ВСЕГДА строки.

! float используется:
- значение метрики
- результат вычислений
- результат PromQL выражений
пример:
http_requests_total 12345
cpu_usage 0.73
request_latency_ms 800

* Instant Vector - может быть:
- набор time series
- одна sample value
- один timestamp (now)
пример: auth_api_hit

* Instant Vector + filters
auth_api_hit{count="1", time_taken="800"}
1. искать метрик на основе metric name, т.е auth_api_hit
2. фильтровать time series по labels: count="1", time_taken="800"
и возвращает для каждого time series по одному значению на текущий момент времени
! НЕ ИСТОРИЮ 
! НЕ ДИАПАЗОН
т.е Instant Vector = фотография

* Range Vector
- набор time series
- много samples
- за указанный период времени
синтаксис:
metric_name[time_range]
пример:
auth_api_hit[5m]

Допустимые единицы времени
ms - milliseconds
s - seconds
m - minutes
h - hours
d - days (24h)
w - weeks (7d)
y - years (365d)
!Нет месяцев!
!Case-sensitive!

Вопросы:
1. Какие data types есть в Prometheus? Scalar, Instant Vector, Range Vector.
2. Что такое instant vector? Набор time series с одним sample на текущий момент.
3. Что такое range vector? Набор time series с samples за период времени.
4. Почему запрос с code=200 может не работать? Потому что значение было сохранено как string.
5. От чего зависит количество значений в range vector? От time range и scrape interval.
6. Можно ли использовать месяцы в range vector? Нет.

11) PromQL arithmetic operators (Scalar, Instant Vector)
+ - addition
- - subtraction
* - multiplication
/ - division
% - remainder
^ - power

* Scalar ⨯ Scalar = результат = scalar
2 + 2 = 4

* Scalar ⨯ Instant Vector = scalar применяется к каждому элементу instant vector
vector = [5, 6]
vector + 5 =>  [10, 11]
!Scalar применяется ко всем значениям vector!

* Instant Vector ⨯ Instant Vector = операция выполняется только для matching time series
time series - m1{label="c"}, metric name => m1 и labels => {label="c"}
matching = одинаковый metric name + одинаковые labels
если нет match - series пропадает
A: m1{label=a}, m1{label=b}, m1{label=c}
B: m1{label=a}, m1{label=b}

A + B => только label=a и label=b
!Нет matching labels => нет результата!

Вопросы:
1. Какие арифметические операторы есть в PromQL? +, -, *, /, %, ^
2. Что происходит при scalar + instant vector? Scalar применяется ко всем элементам vector.
3. Что происходит при instant vector + instant vector? Операция выполняется только для matching time series.
4. Когда time series не попадёт в результат? Если нет matching metric name и labels.
5. Изменяется ли исходный vector при операции? Нет, всегда создаётся новый vector.

12) PromQL binary comparison operators
такие же как и везде... ==, > и тд, и тп..

* Scalar ⨯ Instant Vector = оператор применяется к каждому элементу vector и остаются только те series, где условие true:
vector: [a=10, b=4]
vector == 10 => [a]
!Series, где условие false, исчезают!

* Instant Vector ⨯ Instant Vector = сравниваются только matching time series 
matching = metric name + labels
остаются series, где есть match и условие true.
A: m{a=10}, m{b=4}
B: m{a=10}, m{b=6}
A == B = m{a}

Вопросы:
1. Какие бинарные операторы есть в PromQL? ==, !=, >, <, >=, <=
2. Что возвращает comparison scalar vs scalar? 1 (true) или 0 (false).
3. Что происходит при scalar vs instant vector? Scalar применяется ко всем series, остаются только true.
4. Как работают comparison между двумя vectors? Только matching metric name + labels участвуют.
5. Почему часть series может пропасть? Нет match или условие false.

13) PromQL set binary operators (and / or / unless)
and
A: m{a=10}, m{b=4}
B: m{a=10}, m{c=4}

A and B => m{a}

or
A: m{a}, m{b}
B: m{a}, m{c}

A or B => m{a}, m{b}, m{c}

unless - только элементы из левого vector, которых НЕТ в правом.
A: m{a}, m{b}
B: m{a}, m{c}

A unless B => m{b}

Вопросы:
1. С какими типами данных работают set operators? Только instant vectors.
2. Чувствительны ли они к регистру? Да, всегда lowercase.

11) PromQL aggregation operators = агрегируют один Instant Vector = возвращают новый Instant Vector
!Агрегация всегда создаёт НОВЫЙ vector!
Основные:
sum - сумма значений
min - минимальное значение
max - максимальное значение
avg - среднее значение
count - количество элементов (time series)
count_values - считает элементы с одинаковым значением
group - группирует элементы, value всегда = 1
topk(k, vector) - k наибольших значений
bottomk(k, vector) - k наименьших значений
stddev - стандартное отклонение (population)
stdvar - дисперсия (population)

примеры:
sum(metric_name)
avg(metric_name)
topk(3, metric_name)
sum(metric) by (label)
sum(node_cpu_seconds_total) by (mode)

sum(metric) without (label)
sum(node_cpu_seconds_total) without (mode)
!without = убрать label перед агрегацией!

group(metric)
group(metric) by (label)
!value ВСЕГДА = 1!

topk / bottomk - возвращает только k элементов и часто используется вместе с avg, sum, rate
topk(3, metric)
bottomk(2, metric)
topk(5, avg(http_request_duration_seconds) by (service))

Вопросы:
1. Что делают aggregation operators? Агрегируют Instant Vector => возвращают новый Instant Vector.
2. Разница между by и without? by - группируем по label, without - исключаем label.
3. Для чего нужен group? Только группировка labels, значение всегда 1.
4. Когда использовать topk? Для поиска самых 'тяжёлых' сервисов / хостов / метрик.
5. Можно ли агрегировать Range Vector? Нет, сначала нужен Instant Vector (обычно через rate, avg_over_time).

12) Time offset в PromQL
offset - позволяет смотреть значения метрик в прошлом, а не latest scrape = если нужно значение N минут / часов / дней назад
пример:
metric_name offset 5m
metric_name offset 10m
metric_name offset 8h
metric_name offset 2d

!offset НЕ меняет тип данных

prometheus_http_requests_total            => value = 21 (now)
prometheus_http_requests_total offset 8m  => value = 20 (8 min ago)

правильная запись
avg(prometheus_http_requests_total offset 8h) by (code)

говно из жопы !!!!!
ГОВНО avg(prometheus_http_requests_total) by (code) offset 8h
ГОВНО avg(prometheus_http_requests_total by (code)) offset 8h

!!!offset применяется к метрике, а не к aggregation

* offset + aggregation
ПОРЯДОК: metric offset time => aggregation => by / without
пример:
avg(prometheus_http_requests_total offset 1h) by (code)

!group плохо для графиков
group(metric) by (label)
- value всегда = 1
- график всегда flat
- для графиков нужны sum / avg / count

типикал use-cases offset:
- сравнение now vs past
- baseline comparison
- что было час назад?
- анализ деградации
пример:
rate(http_requests_total[5m]) / rate(http_requests_total[5m] offset 1h)

Вопросы:
1. Где писать offset? Сразу после метрики, до aggregation.
2. Можно ли применять offset к aggregation? Нет

14) 
* PromQL functions (часть 1): absent, math, clamp
параша... нагугли и прочитай, энивей хуй запомнишь

примеры:
проверка, что метрика ПРОПАЛА
absent(node_cpu_seconds_total) => пусто
absent(node_cpu_seconds_total{cpu="random"}) => { } = 1

та ж опера, но с range vector
absent_over_time(node_cpu_seconds_total[1h])

обрезка значений
clamp_min(node_cpu_seconds_total, 300)
clamp_max(node_cpu_seconds_total, 150000)
clamp(node_cpu_seconds_total, 300, 150000)

* PromQL time & delta functions (day, weekday, delta)
day_of_month(metric)
day_of_week(metric)
delta(node_cpu_temperature[2h])
idelta()

* PromQL utility functions: log, sort, time, timestamp
sort(clamp(metric, 300, 150000))
sort_desc(metric)

timestamp(metric)            => now
timestamp(metric offset 1h)  => now - 1h

* Aggregation over time (*_over_time)
avg_over_time(metric[2h])

примерно так работает:
metric           => Instant Vector (now)
metric[2h]       => Range Vector (history)
avg_over_time()  => Instant Vector (1 value per series)

пример:
avg_over_time(node_cpu_seconds_total{cpu="0"}[2h])

15) Prometheus + Windows (WMI / Windows Exporter)
! У Prometheus НЕТ официального exporter’а для Windows
(в отличие от Node Exporter для Linux)
Но существуют third-party exporters, которые основаны на WMI.
WMI (Windows Management Instrumentation) - встроенная инфраструктура Windows для:
- получения системных метрик
- управления ОС
- автоматизации админских задач
!WMI != Prometheus
!Prometheus просто читает данные, которые exporter достаёт из WMI!
Сейчас эта залупа называется windows_exporter, раньше wmi_exporter(многие скуфы до сих пор так называют).

* Архитектура:
Windows OS
  ↓ (WMI)
Windows Exporter (port 9182)
  ↓ (HTTP /metrics)
Prometheus (scrape)

!Prometheus exporter слушает порт 9182 - написано в документации... так что хз, держи за факт.

16) Grafana
Cloud vs Self-hosted
Grafana Cloud = скорость и простота
Self-hosted = контроль и гибкость
!Выбор по архитектурному и бизнес-решению, а не “что моднее”

* Grafana Cloud - буду юзать чисто free tier для пет проектов с целью обучения. PS не хочу лишний раз ебаться, запустил в облаке и сидишь довольный, как слон. 
Из минусов:
- в продакшне бьет по кошельку. Моему? нет... так что меня это ебать не должно, но факт есть факт.
- ограничения на кастомизацию (хз че я там кастомизировать собрался)
- security / data residency(ГДЕ физически хранятся данные)
- GDPR / compliance, где
	GDRP(General Data Protection Regulation) - это закон ЕС о защите персональных данных, он регулирует, где, как и кем могут храниться и обрабатываться данные людей из ЕС.
	Compliance = соответствие законам, стандартам и внутренним правилам компании.
- vendor lock-in - эт когда ты зависим от одного поставщика. Если хочешь ливнуть, то эт больно и дорого. Т.е надо будет после ухода переписывать код, мигрировать данные, менять инфраструктуру.


* Self-hosted Grafana (On-prem) - все сам, развертка, настройка. Из плюсов - кастомизация, потенциально бесплатно и безопасность.

* Установка
типикал сценарии
- VM (AWS / Azure / GCP / on-prem)
- Ubuntu 20.04 / 22.04
- Grafana как systemd service

базовые бибатеки
- adduser
- libfontconfig1
- musl

конфиг на Windows тут 
\grafana\grafana\conf\defaults.ini 
создать копию с названием custom.ini
поменял локал порт на 3300
http_port = 3300

затем рестарт Restart-Service grafana
ну или services.msc => Grafana => restart или stop - start
дефолт юзер и пассворд admin => потом я его поменяю на PizdaKita69

Grafana Docker
https://github.com/aussiearef/grafana-udemy/blob/ba5010d93a9d702dad042d9f572ac0f9b9a92116/docker/docker-compose.yaml
https://github.com/aussiearef/grafana-udemy

17) Grafana dashboards
! Dashboards нужно делать под цель, а не 'всё в одном'

Типы дашбордов и их цель:
* Browser / Frontend dashboards (Angular / React / Web)
Фокус: UX + ошибки + производительность
Рекомендуемый layout:
- Top (самое важное):
Error rate
Number of errors
Top N errors (справа)
- Middle:
Page load time (latency)
Page views / throughput
- Bottom (UX):
Core Web Vitals: LCP(Largest Contentful Paint), FID(First Input Delay), CLS(Cumulative Layout Shift)

* APM(Application Performance Monitoring) dashboards (Backend / Services)
Фокус: здоровье сервиса
Рекомендуемый layout:
- API calls per minute (traffic)
- Error rate
- Logs volume
- Hosts / containers running the service
- CPU usage
- Memory usage

* Infrastructure dashboards
Фокус: состояние платформы
Рекомендуемый layout:
- Summary сверху:
Hosts count
Applications
Events
Alerts / warnings
- Key metrics:
CPU usage
Memory usage
Disk usage / utilization
- Details:
List of hosts / VMs / containers
Databases
Caches (Redis, etc.)
Отвечает на вопрос: Инфраструктура выдерживает нагрузку?

* Synthetic monitoring dashboards
Фокус: доступность извне (black-box)
Synthetic monitoring = без instrumentation ! (ничего не ставим в код / сервис)
Например: Ping website, HTTP HEAD / GET, External dependencies
Рекомендуемый layout:
- Landing pages: UP / DOWN (green / red)
- API health checks
- Page load time
- External systems:
Redis (cloud)
MySQL (cloud)
Kafka
RabbitMQ
Отвечает на вопрос: Сервис вообще доступен пользователю?

* Business dashboards
Фокус: бизнес-метрики, не техметрики
Хуйня, но если будет надо, то обычно делают такие метрики:
- Total sales count
- Refund count
- Sales value
- Refund value
- Comparison:
vs last week
vs last month
- Sales / payments by region
- Conversion rate
- Customer acquisition
- Abandoned checkout
- Top payment methods
- Average basket value
!это не APM, это decision-making dashboards

Запомни принципы: 
один шаблон = одна цель!!!!!
САМОЕ ВАЖНО - ВВЕРХУ
Errors & availability - ПРИОРИТЕТОМ
Не мешать: infra, app, business !!!!
!Dashboard должен отвечать на вопрос, а не 'красиво выглядеть'

Вопросы:
1. Какие типы Grafana dashboards ты знаешь?
- Browser / Frontend
- APM (backend)
- Infrastructure
- Synthetic monitoring
- Business dashboards

5. Чем synthetic monitoring отличается от APM?
- Synthetic => black-box, без instrumentation
- APM => внутри приложения

6. Что должно быть вверху frontend dashboard?
- Error rate
- Errors count
- Performance

7. Что такое business dashboard? Метрики бизнеса, а не системы

8. Почему нельзя делать один dashboard 'на всё'?
- разная аудитория
- разные цели
- разные сигналы


18) Alerting в Grafana
Alerting нужен не для красоты графиков, а для быстрой реакции на проблемы!!!
Dashboard = visibility
Alerting = action

Alert - это сигнал, который срабатывает, когда нарушено правило.
Правило = query + condition

** Alerting architecture
Alert Rule => Notification Policy => Contact Point
!Дуранчелус, если ты не понимаешь эту цепочку - ты не понимаешь Grafana alerting!

1. Alert Rules - определяют КОГДА алерт должен сработать.
основаны на query, имеют condition (>, <, ==, threshold) и ОБЯЗАТЕЛЬНО используют labels!
Labels нужны для маршрутизации уведомлений. 
Пример labels: 
team=backend
service=payments
severity=critical
country=EU

!Без labels alerting превращается в мусор, в помойку, в ненужную, постную хуйню для галочки!

2. Notification Policies - определяют НУЖНО ли отправлять уведомление и КОМУ отправлять.
Работают по labels, а не по имени метрики.
Default policy НЕ ТРОГАТЬ
Всегда создавать nested policies
Пример:
team = payments => send to L2-support
Можно фильтровать по:
team
service
country
severity
любому label

!Notification policy = routing logic!

3. Contact Points - определяют КУДА отправлять алерт.
Примеры:
Email
Slack
Microsoft Teams
PagerDuty
OpsGenie
Telegram
!Contact point сам по себе НИЧЕГО не делает, пока не привязан к policy!

* Email vs Chat tools
Email:
- медленно
- легко пропустить
- плохо для инцидентов
Slack / Teams / PagerDuty:
- мгновенно
- видно всей команде
- есть контекст и ссылки
!Вывод:
Email - только для learning / fallback
Chat tools - для production

ЧИСТО ДЛЯ РАСШИРЕНИЯ КРУГОЗОРА АКА ХУЙНЯ ТЕБЕ ЭТО НЕ НАДО - Slack alerting 
Slack интеграция дел
Flow: Slack Incoming Webhook URL => Grafana Contact Point => Notification Policy => Alert Rule firesается через Incoming Webhook.
Webhook URL: копировать полностью, без токена, универсальный способ (работает и для Teams)

Почему приходит несколько алертов?
Если alert rule: параметризован по labels (country, instance, pod), условие нарушено для каждого значения.
Grafana создаёт ОДИН alert на каждый label-set
Это НОРМАЛЬНО и ПРАВИЛЬНО.

!Best practices:
- Labels - ОСНОВА alerting
- Фильтровать уведомления по labels, не по метрикам
- Default notification policy не менять
- Один alert = одна проблема
- Alert должен требовать действия
- Не алертить на всё подряд

Anti-patterns(хуйня не делай так)
- Один алерт на всё
- Алерты без labels
- Алерты без владельца
- Email-only alerting
- Alert fatigue (слишком много алертов)

Alerting vs Monitoring
Monitoring:
- показывает, что что-то не так
- detection
Alerting:
- требует реакции
- action
Monitoring без alerting = смотришь на графики и ничего не делаешь
Alerting без мониторинга = слепые алерты

Вопросы:
1. Как работает alerting в Grafana? Alert Rule => Notification Policy => Contact Point
2. Зачем нужны labels в алертах? Для маршрутизации уведомлений
3. Почему нельзя слать все алерты всем? Alert fatigue => алерты начинают игнорировать
4. Почему email плох для alerting? Медленно, нет реакции, легко пропустить
5. Чем alerting отличается от dashboard?
Dashboard = visibility
Alerting = action

Кратко по алертам
Alert rules decide WHEN
Notification policies decide WHO
Contact points decide WHERE

19) Grafana Loki - это log aggregation system, заточенный под Grafana.
loki-config.yaml
Он закрывает L (Logs) в MELT.
- Loki не имеет собственного UI
- Логи смотрят ТОЛЬКО через Grafana
- Loki = data source для Grafana

Мне нужен Loki, т.к Observability ≠ только метрики - потому что метрики показывают что сломалось, логи - почему.
Loki решает:
- централизованный сбор логов
- быстрый поиск по логам
- корреляцию логов с метриками и дашбордами в Grafana

Свойства Loki:
Log aggregation - собирает и хранит логи
Prometheus-like модель
- labels
- похожая философия
- свой query language (LogQL)
Tight integration с Grafana
Distributed architecture
- горизонтально масштабируется
Cost-effective storage
- chunk-based storage
- компрессия логов
- дешевле, чем классические log systems

Основной сценарий:
Application
  => writes logs to local files
Agent (Promtail)
  => reads log files
  => pushes logs
Grafana Loki
  => stores logs
Grafana
  => queries Loki
  => visualizes logs

* Promtail - агент для Loki
- агент, который:
- читает лог-файлы
- добавляет labels
- отправляет логи в Loki

работает по конфигу(promtail-config.yaml):
- где лежат логи
- какие файлы читать

!!Promtail = agent
!!Loki = storage + query
!!Grafana = visualization

* Способы отправки логов в Loki
1. Promtail (рекомендуемый)
- backend пишет логи в файл
- promtail читает файл
- promtail пушит в Loki
Плюсы:
+ backend не зависит от Loki
+ стандартный подход
+ отказоустойчиво

2. Push напрямую из приложения
- приложение пушит логи в Loki API
Минусы:
- Loki становится зависимостью приложения
- не рекомендуется для большинства кейсов
ИСПОЛЬЗУЙ только если есть веская причина

3. Grafana Alloy
- более универсальный агент
умеет:
- logs
- metrics
- traces
- OpenTelemetry signals
Подходит если:
- нужна расширяемая observability платформа
- планируется OTel
- сложная архитектура

!!Promtail vs Grafana Alloy
-- Promtail
- проще
- меньше компонентов
- logs-only
- хороший старт

-- Grafana Alloy
- универсальный агент
- future-proof
- поддержка OpenTelemetry
- сложнее, но мощнее

ЕСЛИ КРАТКО ПО ТЕМЕ
Loki = logs backend
Grafana = единственный UI
Promtail = де-факто стандартный агент
Прямой push из приложения - антипаттерн
Alloy - агент будущего, Promtail - рабочая классика

Вопросы: 
1. Что такое Grafana Loki? Log aggregation system для Grafana.
2. Есть ли у Loki собственный UI? Нет.
3. Как обычно логи попадают в Loki? Через Promtail.
4. Почему не рекомендуется пушить логи напрямую из приложения? Tight coupling, зависимость от Loki.
5. Когда нужен Grafana Alloy? Когда нужна расширяемая observability платформа и OpenTelemetry.

OpenTelemetry - это библиотека (SDK) для .NET, которая инструментирует приложение и позволяет стандартизированно отдавать метрики, логи и трейсы во внешние системы observability.

Для монолита обычные метрики + логи.
пример
[ ASP.NET Monolith ]
   | metrics (/metrics)
   | logs (JSON)
   v
Prometheus -----> Grafana
Loki <----- logs

например для логов:
dotnet add package Serilog.AspNetCore - интегрирует Serilog с ASP.NET Core (pipeline, DI, lifecycle приложения).
dotnet add package Serilog.Sinks.Console - выводит логи в консоль (удобно для dev, Docker, Kubernetes).
dotnet add package Serilog.Sinks.File - пишет логи в файлы на диске (для хранения, анализа и отправки в Loki/ELK).

для метриков:
dotnet add package prometheus-net.AspNetCore - интеграция Prometheus с ASP.NET Core.
 
app.UseHttpMetrics(); // HTTP metrics (requests, latency, status codes)
app.MapMetrics(); // /metrics endpoint

Prometheus сам по себе не user-friendly. У него есть примитивный UI только для отладки запросов.
Поэтому почти всегда делают так:

Prometheus  ← PromQL ←  Grafana
   ↑                    ↓
 /metrics            Dashboards / Alerts
 
18) OpenTelemetry (OTel) - vendor-neutral стандарт для генерации и передачи telemetry data.
!OpenTelemetry ≠ backend
!OpenTelemetry = instrumentation + transport
- генерирует Metrics
- генерирует Logs
- генерирует Traces
- экспортирует их в observability backend

Backend примеры:
- Prometheus (metrics)
- Grafana Loki (logs)
- Grafana Tempo / Jaeger (traces)
- New Relic, Splunk и т.д.

Возможные проблемы без OTel:
- vendor lock-in
- разные SDK под каждый backend
- сложно мигрировать

И решением = OTel:
- один стандарт
- одна модель данных
- много backends

OpenTelemetry ≠ Prometheus:
OpenTelemetry				Prometheus
Push-based				Pull-based
Генерирует и экспортирует telemetry	Хранит и обрабатывает metrics
Metrics + Logs + Traces			Только metrics
Стандарт				Backend

C OpenTelemetry обычно работают c двух перспектив:
1. Developer perspective
- есть доступ к коду
- добавляешь SDK
- создаёшь custom metrics / traces / logs
SDK есть для:
.NET, Java, Go, JS, Python, PHP, Ruby, Rust, Swift

2. DevOps perspective
- кода не трогаем
- используем auto-instrumentation
- собираем telemetry извне
Пример:
.NET profiler
Java agent
!Auto-instrumentation = меньше гибкости, но быстро

Архитектура OpenTelemetry (упрощённо ака для ебланов):
* Application
  -> OpenTelemetry SDK / Auto-Instrumentation
      -> Exporter (OTLP / Prometheus / etc)
          -> Collector (optional, но важно)
              -> Backend

* Exporters
Exporter = куда отправлять telemetry
Примеры:
- Prometheus exporter
- OTLP exporter
- New Relic exporter
- Splunk exporter
!!!!OTLP (OpenTelemetry Protocol) - универсальный стандарт
!Будущее именно за OTLP

* OpenTelemetry Collector
Collector нужен, когда:
- много сервисов
- нужен контроль трафика
- нужна агрегация, фильтрация, batching
Collector:
- принимает telemetry
- обрабатывает
- отправляет дальше
!Collector = control plane observability

!!!!!!!Push vs Pull
- OpenTelemetry:
ВСЕГДА push
- Prometheus:
ВСЕГДА pull

OTel => Prometheus = через remote_write
начинать настройку нужно с backend

Prometheus + OpenTelemetry (remote_write)
remote_write - пушить метрики дальше
пример Endpoint: http://<prometheus-host>:<port>/api/v1/write

* Grafana Alloy = Grafana distribution of OpenTelemetry Collector
это  OpenTelemetry Collector +
Prometheus-friendly
Loki-friendly
Tempo-friendly

Возможности:
--принимает metrics / logs / traces
--источники:
	- OpenTelemetry SDK
	- Prometheus exporters
	- Linux / Windows
	- .NET / Java
	- Kubernetes
-- обрабатывает telemetry
-- отправляет:
	- metrics -> Prometheus
	- logs -> Loki
	- traces -> Tempo
!Alloy = единый агент для всего

promtail vs alloy:
promtail	Grafana Alloy
Только логи	Metrics + Logs + Traces
Только Loki	Loki + Prometheus + Tempo
Просто		Универсально
Устаревает	Будущее

Коротко:
promtail - ок для обучения
Alloy - для нормальной архитектуры

Т.е подводя итоги: 
Минимальный сетап
- Metrics -> Prometheus
- Logs -> Loki
- Traces -> потом

Нормальный сетап
- Instrumentation -> OpenTelemetry
- Transport -> Grafana Alloy
- Storage -> Prometheus / Loki / Tempo
- UI -> Grafana

OpenTelemetry - стандарт, не backend
OTel = Metrics + Logs + Traces
OTel работает по push-модели
Collector нужен при масштабе
Grafana Alloy = лучший collector сейчас
Prometheus остаётся backend’ом
Grafana - только UI

!!! Минимальный современный монолит без OpenTelemetry
[ Monolith (.NET) ]
   | metrics (/metrics) - prometheus-net
   | logs (files)
   v
Prometheus  - scrapes /metrics (RPS, Error rate, Latency (P95))
Promtail    - tails log files
   |	
   v
Loki        - logs (structured logs (JSON), correlation id)
Grafana     - UI (Error rate > X, Latency > Y) -> send alerts

!!! Минимальный современный монолит с OpenTelemetry
[ Monolith ]
  └─ OpenTelemetry SDK
        |
        v
   Grafana Alloy
        |
        +--> Prometheus (metrics)
        +--> Loki (logs)
        +--> Tempo (traces, опционально) - хуйня
        |
      Grafana
НО без traces - половина ценности OTel теряется


19) Tracing - нужен только для distributed systems.
прикол https://view.genial.ly/662de2ca7c54340013e64ade

Tracing отвечает на вопрос: где именно запрос замедлился или сломался, когда он проходит через несколько сервисов.
т.е 
Метрики → говорят что плохо
Логи → говорят что произошло
Трейсы → показывают путь запроса и точку проблемы

* Базовые понятия в tracing
- Trace:
Полный путь одного запроса через систему
Имеет trace_id
Состоит из span’ов
- Span
Один шаг работы внутри trace
Например: HTTP запрос, SQL query, вызов сервиса
Имеет span_id
Есть parent / child связи
- Trace context
Метаданные: trace_id, span_id
Передаются между сервисами
- Sampling
Трейсы дорогие
Трассируют не все запросы, а часть
Нужно для снижения нагрузки и объёма данных

* !!! tracing в microservices!!!
- Без tracing:
невозможно понять, где именно запрос тормозит
невозможно понять, какой сервис виноват
root cause искать долго и больно

- Tracing позволяет:
найти bottleneck
увидеть зависимости сервисов
понять latency breakdown
построить service graph

* Инструменты tracing (кто есть кто)
- Instrumentation (создают трейсы)
OpenTelemetry (стандарт) - sdk для того же .NET
Zipkin
New Relic SDK

- Backends (хранят и показывают)
Jaeger
Zipkin
Grafana Tempo - эту залупу я использую

* Grafana Tempo - подключается к Tempo, визуализирует трейсы, использует TraceQL.
Дальше будет тоже самое, только с заумными словами:
- tracing backend
- open-source
- масштабируемый
- НЕ требует БД
- хранит трейсы в: local disk или S3 / GCS / Azure Blob

!!!!!Рекомендованная современная архитектура
Application
  ↓ (OTLP)
Grafana Alloy (collector)
  ↓
Grafana Tempo (traces)
  ↓
Grafana (UI)

где Alloy = единая точка приёма,
batching, filtering, processing, 
не привязываем код напрямую к Tempo.

* Context propagation — ключевая идея tracing
Tracing не работает автоматически, нужно передавать trace_id и span_id между сервисами
- HTTP: trace context → HTTP headers
- Message broker: trace context → message payload / headers

* Client / Server spans (важно для service graph)
HTTP вызов → SpanKind.Client
Приём HTTP → SpanKind.Server
!!! Без этого service graph не построится

* Service Graph в Grafana Tempo
Service Graph показывает:
- какие сервисы вызывают друг друга
- RPS
- ошибки
- latency между сервисами

!!! Service Graph использует Prometheus
Tempo сам экспортирует metrics → Prometheus (через remote_write)
!!!! Если нет Prometheus service graph работать не будет

!!!! Tempo + Prometheus 
Tempo config:
- включает remote_write
- отправляет derived metrics в Prometheus
- Prometheus используется только для service graph

* TraceQL — очередной, сука, язык запросов Tempo… бляяя… как же эта вся хуйня остаебенила ебанарот…
TraceQL вдохновился PromQL, но блять… почему тогда не придумать единый формат! нахуя мне то и то в голове держать... айтишники ебаные...

Можно фильтровать по:
- span name
- duration
- status
- service name
- attributes

Например:
найти медленные запросы
найти ошибки
найти конкретный сервис
найти trace с N spans
и тд, и тп

Вкратце и по существу:
Tracing нужен для distributed systems
Trace = путь запроса
Span = шаг внутри пути
Context propagation — must have
OpenTelemetry — стандарт для tracing
Grafana Tempo — backend для трейсов
Grafana Alloy — рекомендованный collector
Service Graph требует Prometheus
Без client/server spans граф не работает
Tracing ≠ monitoring, но без него microservices — слепые